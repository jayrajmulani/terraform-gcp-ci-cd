worker_processes 1;
error_log logs/error.log;
events {
    worker_connections 1024;
}

http {
    lua_package_path "/path/to/lua-resty-upstream-healthcheck/lib/?.lua;;";

    # List of servers that the service will load balance
    upstream backend {
        server {{ external_ip }};
    }

    # Rate Limit Zone - Define a key zone for rate limiting
    limit_req_zone $binary_remote_addr zone=coffeelimit:10m rate=10r/s;

    # the size depends on the number of servers in upstream {}:
    lua_shared_dict healthcheck 1m;

    lua_socket_log_errors off;

    init_worker_by_lua_block {
        local hc = require "resty.upstream.healthcheck"

        local ok, err = hc.spawn_checker {
            shm = "healthcheck",  -- defined by "lua_shared_dict"
            upstream = "backend", -- defined by "upstream"
            type = "http", -- support "http" and "https"

            http_req = "GET / HTTP/1.0\r\nHost: foo.com\r\n\r\n",
                    -- raw HTTP request for checking

            port = nil,  -- the check port, it can be different than the original backend server port, default means the same as the original backend server
            interval = 2000,  -- run the check cycle every 2 sec
            timeout = 1000,   -- 1 sec is the timeout for network operations
            fall = 3,  -- # of successive failures before turning a peer down
            rise = 2,  -- # of successive successes before turning a peer up
            valid_statuses = {200, 302},  -- a list valid HTTP status code
            concurrency = 10,  -- concurrency level for test requests
            -- ssl_verify = true, -- https type only, verify ssl certificate or not, default true
            -- host = foo.com, -- https type only, host name in ssl handshake, default nil
        }
        if not ok then
            ngx.log(ngx.ERR, "failed to spawn health checker: ", err)
            return
        end

        -- Rate Limiting Initialization
        local lim = require "resty.limit.req"
        local limit, err = lim.new("coffeelimit", 10, 5)
        if not limit then
            ngx.log(ngx.ERR, "failed to instantiate a resty.limit.req object: ", err)
            return
        end
        ngx.ctx.limit_req = limit
    }

    server {
        listen 80;
        listen 443;

        # Any route coming to this server will be proxied to the list of backend servers(default: round-robin)
        location / {
            
            # Rate Limiting
            limit_req zone=coffeelimit burst=5 nodelay;
            limit_req_status 429; # Set status code 429 (Too Many Requests) for exceeded limit

            proxy_pass http://backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
        }
        
        # status page for all the peers:
        location = /status {
            access_log off;

            default_type text/plain;
            content_by_lua_block {
                local hc = require "resty.upstream.healthcheck"
                ngx.say("Nginx Worker PID: ", ngx.worker.pid())
                ngx.print(hc.status_page())
            }
        }

        # status page for all the peers (prometheus format):
        location = /metrics {
            access_log off;
            default_type text/plain;
            content_by_lua_block {
                local hc = require "resty.upstream.healthcheck"
                st , err = hc.prometheus_status_page()
                if not st then
                    ngx.say(err)
                    return
                end
                ngx.print(st)
            }
        }
    }
}